<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Web Audio Api</title>
  <link rel="stylesheet" href="./css/night.css" />
  <script src="./javascript/jquery-3.2.1.min.js"></script>
  <script>
    $(document).on('click', 'a[href^="#"]', function (event) {
      event.preventDefault();

      $('html, body').animate({
        scrollTop: $($.attr(this, 'href')).offset().top
      }, 500);
    });
  </script>
</head>

<body>
  <div class="custom-navigation">
    <ul>
      <li><a href="#getting-started-with-web-audio-api">Начало</a></li>
      <li><a href="#audiocontext">AudioContext</a></li>
      <li><a href="#audionode">AudioNode</a></li>
      <li><a href="#аудио-източници">Аудио източници</a></li>
      <li><a href="#цитирана-литература">Цитирана литература</a></li>
      <li><a href="#списък-с-примерен-програмен-код">Списък с примерен програмен код</a></li>
    </ul>
  </div>

  <div class="container">
    <h1 id="getting-started-with-web-audio-api">Getting Started with Web Audio API</h1>

    <p>Спецификацията за <code>Web Audio Api</code> описва програмен интерфейс от високо ниво за генериране, обработка и представяне
      на зукови източници в съвременните браузъри поддържащи този стандарт. В основата му стоят няколко абстрактни структури
      чрез които той построява граф на обработката на аудиото. В главната нишка на браузъра се извършват всички “контролни”
      извиквания на функции от интерфейсът, докато самата обработка на звука се извършва в бекграунд нишка. Разделени по
      този начин функционалностите, се предоставя възможност обработката на звука да бъде реализирана на много ниско програмно
      ниво, което допринася за нейната оптимизация и голям ансамбъл от възможности [<a target="_blank" href="https://webaudio.github.io/web-audio-api/#abstract">1</a>].</p>

    <p>Нека сега разгледаме някои от основните компоненти на този аудио граф.</p>



    <h2 id="audiocontext">AudioContext</h2>

    <p>Всичко свързано с аудио графа се извършва в нещо наречено <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext">AudioContext</a></strong>.
      Различните имплементации на този стандарт предоставят различен достъп до този контекст затова е нужно в някои случаи
      да имаме префикс <code>webkit</code></p>



    <pre class="prettyprint"><code class="language-javascript hljs "><span class="hljs-keyword">let</span> audioContext = <span class="hljs-keyword">new</span> (window.AudioContext || window.webkitAudioContext)(); </code></pre>

    <p class="figure-description"><strong>Код 1.</strong> мулти-браузърно решение за съдаване на нов аудио контекст</p>

    <p>Чрез този контекст се задават някои мета-данни за изпълнението на музикалните източници. Също така чрез функциите <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/suspend">AudioContex.suspend</a></strong>      и <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/resume">AudioContext.resume</a></strong>      може да се контролира спирането и пускането на звуците. Друга полезна функция е <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/getOutputTimestamp">AudioContext.getOutputTimestamp</a></strong>      която връща данни за текущото време на изпълнение, както и за времето когато реално ще прозвучи даденият звук. Чрез
      нея може например да се провери дали дадена песен приключила изпълнение, или пък да се прави сложна схема за синхронизиране
      на звуците в игра например (или някакъв интерактивен уебсайт).</p>

    <p>В повечето случаи един такъв контекст е достатъчен за цялото приложение, защото един контекст задава един граф за обработка
      на аудио. Също така по спецификация на аудио контекстът му е “позволено” да започне обработка, само ако браузърът позволява
      излъчването не резултатният сигнал към потребителят. С други думи ако сме забранили звукът всичките изчисления дори
      няма да се случат [<a target="_blank" href="https://webaudio.github.io/web-audio-api/#AudioContext">2</a>]. </p>



    <h2 id="audionode">AudioNode</h2>

    <p>Аудио елементите са изграждащите елементи на аудио графа. Те могат да бъдат от няколко основни типа: крайни елементи(елемент
      който има само вход), източници или междинни елементи. Всеки тип елемент се различава по това как обработва подадената
      му информация. Елементите могат да имат 0 или повече входа, и 0 или повече изхода. Всеки вход/изход има точно определен
      брой канали(1 канал отговаря на моно звук, повече канали е стерео). При случаят на множество входове/изходи се извършва
      “миксиране” на каналите [<a target="_blank" href="https://webaudio.github.io/web-audio-api/#channel-up-mixing-and-down-mixing">3</a>].
      Също така дори и да няма входове, елементът пак поддържа един канал който е “мълчалив”, тоест излъчва сигнал само от
      <code>0</code>. <br> Всички аудио елементи трябва да имплементират зададеният в спецификацията интерфейс на елемент
      от аудио графа <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode">AudioNode</a></strong>.
      <br> Свързването на аудио елементите се извършва чрез функцията <strong><a target="_blank" href="%28https://developer.mozilla.org/en-US/docs/Web/API/AudioNode%29">AudioNode.connect</a></strong>.
      Извиква се от елементът чийто изход искаме да свържем към входа на друг елемент.</p>



    <h3 id="крайни-елементи">Крайни елементи</h3>

    <p>Това представлява връзката с устройството което ще изпълнява музикалните файлове. Най-често това са говорителите или
      слушалките на потребителят, но това не е задължително. Могат да бъдат достъпени чрез <code>audioContext.destination</code></p>



    <h3 id="аудио-източници">Аудио източници</h3>

    <p>Това най-често са елементи които наследяват <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioScheduledSourceNode">AudioScheduledSourceNode</a></strong>.
      Те нямат вход, но имат изход който може да бъде свързан към други елементи. Могат да бъдат както буфери съдържащи байтове
      аудио данни, така и “устройства” които генерират звук на базата на няколко параметъра. Един от елементите който се
      използва за създаване на звук е <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/OscillatorNode">OscillatorNode</a></strong>.
      Представлява периодична вълна и се използва за създаване на една тоналност. Нека сега разгледаме аудио буферите.</p>



    <h4 id="audiobuffersourcenode">AudioBufferSourceNode</h4>

    <p><strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode">AudioBufferSourceNode</a></strong>      съдържат в себе си <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer">AudioBuffer</a></strong>      което представлява масив от байтове които са декодирани, подготвени за изпълнение и скалирани спрямо честотата на изпълнението
      и някои други параметри. <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer">AudioBuffer</a></strong>      може да бъде създаден чрез <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/ArrayBuffer">ArrayBuffer</a></strong>      по следният начин.</p>



    <pre class="prettyprint"><code class="language-javascript hljs "><span class="hljs-keyword">let</span> audioContext = <span class="hljs-keyword">new</span> AudioContext();
<span class="hljs-keyword">let</span> data = <span class="hljs-keyword">new</span> <span class="hljs-built_in">ArrayBuffer</span>(<span class="hljs-number">1024</span>);
<span class="hljs-keyword">let</span> audioBuffer: AudioBuffer;

audioContext.decodeAudioData(data)
  .then(decoded =&gt; audioBuffer = decoded);</code></pre>

    <p class="figure-description"><strong>Код 2.</strong> декодиране на буфер с данни до PCM сигнал</p>

    <p>Примерен <code>snippet</code> който представлява най-простото възможно използване на WebApi за възпроизвеждане на звук.
      С този код се създава нов аудио граф и се задават връзките между отделните му елементи.</p>



    <pre class="prettyprint"><code class="language-javascript hljs "><span class="hljs-keyword">let</span> audioContext = <span class="hljs-keyword">new</span> AudioContext();
<span class="hljs-keyword">let</span> source = audioContext.createBufferSource();
source.connect(audioContext.destination);</code></pre>

    <p class="figure-description"><strong>Код 3.</strong> създаване на аудио граф</p>

    <p>Сега за да изпълним създаденият преди малко буфер е достатъчно просто да го зададем като буфер на <code>source</code>.</p>



    <pre class="prettyprint"><code class="language-javascript hljs ">source.buffer = audioBuffer;
source.start();</code></pre>

    <p class="figure-description"><strong>Код 4.</strong> изпълнение на декодиран буфер през вече създаден аудио граф</p>



    <h3 id="междинни-елементи">Междинни елементи</h3>

    <p>Те могат да бъдат от всякакъв тип. Реализирани са елементи за филтриране на дадена чистота (), елементи за усилване на
      звука(), елементи за извличане на информация от аудио данните чрез транформация на фурие ( - могат да се използват
      за различни визуализации на аудио данните). Също така, всеки може да напише свой собствен елемент и да го интегрира
      с аудио графът:</p>



    <pre class="prettyprint"><code class="language-javascript hljs "><span class="hljs-keyword">class</span> Pupesh extends AudioNode { }

<span class="hljs-keyword">let</span> pupeshNode = <span class="hljs-keyword">new</span> Pupesh()
source.connect(pupeshNode);
pupeshNode.connect(audioContext.destination);</code></pre>

    <p class="figure-description"><strong>Код 5.</strong> примерно дефиниране на нов аудио елемент</p>



    <h3 id="цитирана-литература">Цитирана литература</h3>



    <pre class="prettyprint"><code class="language-markdown hljs ">[1] Paul Adenot, Raymond Toy 
Web Audio API W3C Editor's Draft, публикувано на 08 Декември 2017
Секция: "Abstract"</code></pre>



    <pre class="prettyprint"><code class="language-markdown hljs ">[2] Paul Adenot, Raymond Toy 
Web Audio API W3C Editor's Draft, публикувано на 08 Декември 2017
Секция: "AudioContext"</code></pre>



    <pre class="prettyprint"><code class="language-markdown hljs ">[3] Paul Adenot, Raymond Toy 
Web Audio API W3C Editor's Draft, публикувано на 08 Декември 2017
Секция: "channel-up-mixing-and-down-mixing"</code></pre>



    <h3 id="списък-с-примерен-програмен-код">Списък с примерен програмен код</h3>

    <ul>
      <li><strong>Код 1.</strong> мулти-браузърно решение за съдаване на нов аудио контекст</li>
      <li><strong>Код 2.</strong> декодиране на буфер с данни до PCM сигнал</li>
      <li><strong>Код 3.</strong> създаване на аудио граф</li>
      <li><strong>Код 4.</strong> изпълнение на декодиран буфер през вече създаден аудио граф</li>
      <li><strong>Код 5.</strong> примерно дефиниране на нов аудио елемент</li>
    </ul>
  </div>
</body>

</html>